create_tables.sql -> creates the necessary tables with appropriate columns as described in lost_req.pdf. The data model provided there offered the structure and declaring all tables. the database, which for me is named homework2, must be created using the createdb command before executing this sql script.

gen_insert.py -> opens the legacy csv files and writes sql statements to stdout stream. I couldn't figure it out though:( Trying to match different kinds of data in the legacy files to the tables created by create_tables.sql was too challenging. Moreover, I don't know how to react to primary keys. Surely those aren't created at random, but say there is different information from a different file... how do I match that to primary keys in a different table? If I were able to finish parsing the data, I would write the remaining functions that parse all the data properly and write the insert statements into stdout.

import_data.sh -> curls the legacy files locally. unzips them, and then calls gen_insert.py to parse the data. The sql statements generated by the python script are piped to a new sql file. By executing that sql files, the data actually gets added into the database. Then it cleans up after itself by getting rid of unnecessary sql files.
